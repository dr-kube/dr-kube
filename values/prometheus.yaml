# https://artifacthub.io/packages/helm/prometheus-community/prometheus
# ============================================
# Prometheus Server
# ============================================
server:
  enabled: true

  # Tempo metrics generator가 remote write로 span 메트릭 전송
  extraFlags:
    - web.enable-remote-write-receiver

  securityContext:
    runAsUser: 0
    runAsNonRoot: false
    runAsGroup: 0
    fsGroup: 0

  service:
    type: ClusterIP
    port: 9090

  persistentVolume:
    enabled: true
    storageClass: "standard"
    accessModes:
      - ReadWriteOnce
    size: 8Gi
    mountPath: /data

  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    hosts:
      - prometheus.drkube.local
      - prometheus-drkube.huik.site
    paths:
      - /
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus-drkube.huik.site

# ============================================
# Alertmanager
# ============================================
alertmanager:
  enabled: true

  persistence:
    enabled: true
    storageClass: "standard"
    accessModes:
      - ReadWriteOnce
    size: 2Gi
    mountPath: /data

  ingress:
    enabled: true
    className: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    hosts:
      - host: alert.drkube.local
        paths:
          - path: /
            pathType: Prefix
      - host: alert-drkube.huik.site
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: alertmanager-tls
        hosts:
          - alert-drkube.huik.site

  # Slack webhook Secret 마운트
  extraSecretMounts:
    - name: slack-webhook
      mountPath: /etc/alertmanager/secrets/slack-webhook
      secretName: alertmanager-slack-webhook
      readOnly: true

  # Alertmanager webhook 설정
  config:
    global:
      resolve_timeout: 5m
      slack_api_url_file: /etc/alertmanager/secrets/slack-webhook/url
    route:
      receiver: "slack"
      group_by: ["alertname", "namespace", "pod"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 2h
      routes:
        - matchers:
            - severity="critical"
          receiver: "slack"
          group_wait: 5s
          group_interval: 2m
          repeat_interval: 30m
          continue: true
        # Agent 비용 제어: critical만 저빈도로 웹훅 전달
        - matchers:
            - severity="critical"
          receiver: "agent-critical"
          # 복합 인시던트 수집을 위해 alertname/pod 단위 분리를 줄임
          group_by: ["namespace", "severity"]
          group_wait: 90s
          group_interval: 10m
          repeat_interval: 6h
        - matchers:
            - severity="warning"
          receiver: "slack"
          group_wait: 15s
          group_interval: 10m
          repeat_interval: 2h
        - matchers:
            - severity="info"
          receiver: "null"
          group_wait: 1m
          group_interval: 30m
          repeat_interval: 6h
    inhibit_rules:
      - source_matchers:
          - severity="critical"
        target_matchers:
          - severity="warning"
        equal: ["namespace", "pod"]
    receivers:
      # Agent 전용 receiver (LLM 비용 절약을 위해 critical만 사용)
      - name: "agent-critical"
        webhook_configs:
          - url: "http://dr-kube-agent.monitoring.svc.cluster.local:8080/webhook/alertmanager"
            send_resolved: false  # firing만 분석
      # Slack만 (필요시 사용)
      - name: "slack"
        slack_configs:
          - channel: "#alert"
            send_resolved: true
            title: '{{ if eq .Status "firing" }}:fire: {{ .CommonLabels.alertname }}{{ else }}:white_check_mark: {{ .CommonLabels.alertname }} 해결{{ end }}'
            text: >-
              *{{ .CommonAnnotations.summary }}*
              {{ .CommonAnnotations.description }}
              *Namespace:* {{ .CommonLabels.namespace }}
              *Pod:* {{ .CommonLabels.pod }}
      - name: "null"

# ============================================
# Node Exporter (DaemonSet)
# ============================================
prometheus-nodeExporter:
  enabled: true

  service:
    type: ClusterIP
    port: 9100

  tolerations: []
  hostNetwork: false
  hostPID: false

  extraArgs: []
  podLabels: {}
  resources: {}

# ============================================
# Pushgateway (Off)
# ============================================
prometheus-pushgateway:
  enabled: false

# ============================================
# Prometheus Configuration
# ============================================
serverFiles:
  prometheus.yml:
    scrape_configs:
      # 기본 Prometheus self-scrape (이름 변경: 기본 chart와 중복 방지)
      # NOTE: Helm chart 기본값에 이미 prometheus job이 있으므로 제거
      # - job_name: "prometheus-self"
      #   static_configs:
      #     - targets: ["localhost:9090"]
      # Kube-State-Metrics 스크랩
      - job_name: "kube-state-metrics"
        static_configs:
          - targets:
              - "prometheus-kube-state-metrics.monitoring.svc.cluster.local:8080"

      # Node Exporter 자동 스크랩
      - job_name: "node-exporter"
        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          # 1) node-exporter 서비스만 골라잡기
          #    서비스에 app.kubernetes.io/name=prometheus-node-exporter 라벨이 있다고 가정
          - source_labels:
              [__meta_kubernetes_service_label_app_kubernetes_io_name]
            regex: "prometheus-node-exporter"
            action: keep

          # 2) metrics 포트만 사용 (보통 포트 이름이 metrics 또는 http-metrics)
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: "metrics|http-metrics"
            action: keep

          # 3) node 이름을 라벨로 붙이고 싶으면 (옵션)
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: "node"

      # NGINX Ingress Controller 메트릭 스크랩
      - job_name: "nginx-ingress"
        static_configs:
          - targets:
              - "ingress-nginx-controller-metrics.ingress-nginx.svc.cluster.local:10254"

      # 3. cAdvisor (kubelet) 통해 Pod/컨테이너 메트릭 스크랩
      - job_name: "cadvisor"
        scheme: https
        metrics_path: /metrics/cadvisor

        # 각 Node의 kubelet을 API 서버 proxy로 타고 가는 패턴
        kubernetes_sd_configs:
          - role: node

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true

        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        relabel_configs:
          # 노드 라벨을 Prometheus 라벨로 복사
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

          # 실제 요청은 apiserver로 보내고,
          # metrics_path로 어느 노드인지 지정 (proxy 사용)
          - target_label: __address__
            replacement: kubernetes.default.svc:443

          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

      # OTel Collector span 메트릭 (Grafana Drilldown Traces 용)
      - job_name: "otel-spanmetrics"
        scrape_interval: 15s
        static_configs:
          - targets:
              - "otel-jaeger-forwarder.online-boutique.svc.cluster.local:9464"
  # ===========================================
  # alert rule 설정
  # ============================================
  alerting_rules.yml:
    groups:
      - name: container-oom.rules
        rules:
          - alert: ContainerOOMKilled
            expr: |
              increase(kube_pod_container_status_restarts_total[5m]) >= 1
              and on (namespace, pod, container)
              max_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[5m]) == 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: "Container OOMKilled 발생"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }}
                컨테이너 {{ $labels.container }} 가
                최근 5분 내 OOMKilled로 인해 재시작되었습니다.

      - name: container-cpu.rules
        rules:
          - alert: CPUThrottling
            expr: |
              (
                sum by (namespace, pod, container) (
                  rate(container_cpu_cfs_throttled_periods_total{container!="",container!="POD"}[5m])
                )
                /
                clamp_min(
                  sum by (namespace, pod, container) (
                    rate(container_cpu_cfs_periods_total{container!="",container!="POD"}[5m])
                  ),
                  1
                )
              ) > 0.7
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "CPU Throttling 발생"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }}
                컨테이너 {{ $labels.container }} 가
                CPU throttling 비율 70% 이상입니다.

      - name: container-crash.rules
        rules:
          - alert: PodCrashLooping
            expr: |
              increase(kube_pod_container_status_restarts_total[10m]) >= 3
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: "Pod CrashLoopBackOff 발생"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }}
                컨테이너 {{ $labels.container }} 가
                최근 10분 내 3회 이상 재시작되었습니다.
      - name: container-memory.rules
        rules:
          - alert: HighMemoryUsage
            expr: |
              (
                container_memory_working_set_bytes{container!="",container!="POD"}
                / container_spec_memory_limit_bytes{container!="",container!="POD"}
              ) > 0.92
              and on (namespace, pod, container)
              container_spec_memory_limit_bytes{container!="",container!="POD"} > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "메모리 사용량 임계치 초과"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }}
                컨테이너 {{ $labels.container }} 의
                메모리 사용률이 92%를 초과했습니다.

      - name: pod-ready.rules
        rules:
          - alert: PodNotReady
            expr: |
              kube_pod_status_ready{condition="true"} == 0
              and on (namespace, pod)
              kube_pod_status_phase{phase=~"Succeeded|Failed"} == 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod Ready 상태 아님"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }} 가
                Ready 상태가 아닙니다.

      - name: container-waiting.rules
        rules:
          - alert: ContainerWaiting
            expr: |
              kube_pod_container_status_waiting_reason{
                reason=~"CrashLoopBackOff|ImagePullBackOff|ErrImagePull|CreateContainerConfigError"
              } > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "컨테이너 대기 상태 감지"
              description: |
                Pod {{ $labels.namespace }}/{{ $labels.pod }}
                컨테이너 {{ $labels.container }} 가
                대기 상태입니다. (reason={{ $labels.reason }})

      - name: deployment-replicas.rules
        rules:
          - alert: DeploymentReplicasMismatch
            expr: |
              kube_deployment_spec_replicas
              !=
              kube_deployment_status_replicas_available
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Deployment 레플리카 불일치"
              description: |
                Deployment {{ $labels.namespace }}/{{ $labels.deployment }} 의
                spec_replicas 와 available_replicas 가 일치하지 않습니다.

      - name: node-cpu.rules
        rules:
          - alert: NodeHighCPU
            expr: |
              100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "노드 CPU 사용률 높음"
              description: |
                노드 {{ $labels.instance }} 의
                CPU 사용률이 90%를 초과했습니다.

      - name: service-level.rules
        rules:
          - alert: ServiceHighLatencyP99
            expr: |
              histogram_quantile(
                0.99,
                sum by (le, service) (rate(traces_spanmetrics_latency_bucket[5m]))
              ) > 3
              and on (service)
              sum by (service) (rate(traces_spanmetrics_calls_total[5m])) > 0.2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "서비스 지연 증가 (p99)"
              description: |
                서비스 {{ $labels.service }} 의 p99 지연이 3s를 초과했습니다.

          - alert: ServiceHighErrorRate
            expr: |
              (
                sum by (service) (rate(traces_spanmetrics_calls_total{status_code="STATUS_CODE_ERROR"}[5m]))
                /
                sum by (service) (rate(traces_spanmetrics_calls_total[5m]))
              ) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "서비스 에러율 증가"
              description: |
                서비스 {{ $labels.service }} 의 에러율이 5%를 초과했습니다.

          - alert: ServiceDown
            expr: |
              sum by (service) (rate(traces_spanmetrics_calls_total[5m])) == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: "서비스 다운"
              description: |
                서비스 {{ $labels.service }} 에서 10분간 트래픽이 감지되지 않았습니다.

          - alert: UpstreamConnectionError
            expr: |
              (
                sum by (service, peer_service) (
                  rate(traces_spanmetrics_calls_total{span_kind="SPAN_KIND_CLIENT", status_code="STATUS_CODE_ERROR"}[5m])
                )
                /
                clamp_min(
                  sum by (service, peer_service) (
                    rate(traces_spanmetrics_calls_total{span_kind="SPAN_KIND_CLIENT"}[5m])
                  ),
                  0.1
                )
              ) > 0.2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "업스트림 연결 실패"
              description: |
                {{ $labels.service }} -> {{ $labels.peer_service }} 업스트림 연결 에러율이 20%를 초과했습니다.

      - name: nginx-ingress.rules
        rules:
          - alert: NginxHighLatency
            expr: |
              histogram_quantile(0.99, sum by (le) (rate(nginx_ingress_controller_request_duration_seconds_bucket[5m]))) > 7
              and sum(rate(nginx_ingress_controller_requests[5m])) > 0.5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "NGINX Ingress 지연 증가 (p99)"
              description: |
                NGINX Ingress 요청 p99 지연이 5s를 초과했습니다.

          - alert: NginxHigh4xxRate
            expr: |
              sum(rate(nginx_ingress_controller_requests{status=~"4.."}[5m]))
              /
              sum(rate(nginx_ingress_controller_requests[5m]))
              > 0.20
              and sum(rate(nginx_ingress_controller_requests[5m])) > 1
            for: 10m
            labels:
              severity: info
            annotations:
              summary: "NGINX Ingress 4xx 증가"
              description: |
                NGINX Ingress 4xx 비율이 20%를 초과했습니다.

          - alert: NginxHigh5xxRate
            expr: |
              sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m]))
              /
              sum(rate(nginx_ingress_controller_requests[5m]))
              > 0.05
              and sum(rate(nginx_ingress_controller_requests[5m])) > 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "NGINX Ingress 5xx 증가"
              description: |
                NGINX Ingress 5xx 비율이 5%를 초과했습니다.

  # alerting_rules.yml:
  #   groups:
  #   - name: container-oom.rules
  #     rules:
  #       - alert: ContainerOOMKilled
  #         expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
  #         for: 1m
  #         labels:
  #           severity: critical
  #         annotations:
  #           summary: "Container OOMKilled 발생"
  #           description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} 컨테이너 {{ $labels.container }} 가 OOMKilled 되었습니다."
