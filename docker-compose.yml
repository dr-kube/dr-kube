version: "3.8"

services:
  # DR-Kube Agent 개발 환경
  agent:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: dr-kube-agent
    working_dir: /workspace
    volumes:
      # 프로젝트 전체 마운트
      - .:/workspace
      # Python 패키지 캐시 (빌드 속도 향상)
      - pip-cache:/root/.cache/pip
      # kubectl 설정 공유 (로컬 kubeconfig 사용)
      - ${HOME}/.kube:/root/.kube:ro
      # ArgoCD 설정 공유
      - ${HOME}/.argocd:/root/.argocd:ro
    environment:
      # .env 파일에서 환경변수 로드
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:14b}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-2.5-flash}
      # Ollama 컨테이너 연결
      - OLLAMA_HOST=http://ollama:11434
    command: sleep infinity
    networks:
      - dr-kube-network

  # Ollama (로컬 LLM 서버)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: dr-kube-ollama
  #   volumes:
  #     # 모델 데이터 저장 (다운로드한 모델 유지)
  #     - ollama-data:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   networks:
  #     - dr-kube-network
  #   # GPU 사용 시 주석 해제 (NVIDIA GPU 필요)
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

volumes:
  # Python 패키지 캐시 볼륨
  pip-cache:
  # Ollama 모델 데이터 볼륨
  ollama-data:

networks:
  dr-kube-network:
    driver: bridge
